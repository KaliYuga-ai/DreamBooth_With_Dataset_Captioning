{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaliYuga-ai/DreamBooth_With_Dataset_Captioning/blob/main/DreamBooth_With_Dataset_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KaliYuga's DreamBooth With Dataset Captioning\n",
        "\n",
        "<div>\n",
        "<img src=\"https://images.squarespace-cdn.com/content/v1/6213c340453c3f502425776e/a432c21c-bb12-4f38-b5e2-1c12a3c403f6/Animated-Logo_1.gif\" width=\"150\"/>\n",
        "</div>\n",
        "\n",
        "This is KaliYuga's fork of Shivam Shrirao's [DreamBooth implementation](https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth). It adds a number of new features to make dataset labeling and organization faster and more powerful, and training more accurate (hopefully!).\n",
        "\n",
        "**This fork adds the following:** \n",
        "\n",
        "*   a slightly modified version of the ***BLIP dataset\n",
        "autocaptioning functionality*** from [victorchall's EveryDream comapnion tools repo](https://github.com/victorchall/EveryDream).\n",
        "\n",
        "Once you've autocaptioned your datasets, you can use this same notebook to train Stable Diffusion models on your new text/image pairs (with or without instance and class prompts) using \n",
        "\n",
        "*   ***KaliYuga's Dataset Organization Omnitool***, which I wrote with copious help from ChatGPT. This tool lets you extract .txt files from your image filenames so you can train on unique text/image pairs instead of using a single broad instance prompt for a whole group of images. You can still use class and instance prompts alongside the text/image pairs if you want, and this can be a good way to hyper-organize your training data. More detail is given in the Omnitool section.\n",
        "\n",
        "------\n",
        "You can support victorchall's awesome EveryDream on \n",
        "[Patreon](https://www.patreon.com/everydream) or at [Kofi](https://ko-fi.com/everydream)!\n",
        "\n",
        "[Follow Shivam](https://twitter.com/ShivamShrirao) on Twitter\n",
        "\n",
        "[Follow me](https://twitter.com/KaliYuga_ai) (KaliYuga) on Twitter!\n"
      ],
      "metadata": {
        "id": "23tTJ4fFR2Of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------"
      ],
      "metadata": {
        "id": "ntco5RiqRzmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "zlSGHf9taoAA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XU7NuMAA2drw"
      },
      "outputs": [],
      "source": [
        "#@markdown Check type of GPU and VRAM available.\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnTMyW41cC1E"
      },
      "source": [
        "### Install Requirements/Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLWXPZqjsZVV"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/examples/dreambooth/train_dreambooth.py\n",
        "!wget -q https://github.com/ShivamShrirao/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
        "%pip install -qq git+https://github.com/ShivamShrirao/diffusers\n",
        "%pip install -q -U --pre triton\n",
        "%pip install -q accelerate transformers ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4lqqWT_uxD2",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #Login to HuggingFace ðŸ¤—\n",
        "\n",
        "#@markdown You need to accept the model license before downloading or using the Stable Diffusion weights. Please, visit the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5), read the license and tick the checkbox if you agree. You have to be a registered user in ðŸ¤— Hugging Face Hub, and you'll also need to use an access token for the code to work.\n",
        "# https://huggingface.co/settings/tokens\n",
        "!mkdir -p ~/.huggingface\n",
        "HUGGINGFACE_TOKEN = \"\" #@param {type:\"string\"}\n",
        "!echo -n \"{HUGGINGFACE_TOKEN}\" > ~/.huggingface/token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------"
      ],
      "metadata": {
        "id": "PVZIkeK2RkZU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0NV324ZcL9L"
      },
      "source": [
        "## Settings and run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Rxg0y5MBudmd"
      },
      "outputs": [],
      "source": [
        "#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).\n",
        "save_to_gdrive = True #@param {type:\"boolean\"}\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Name/Path of the initial model.\n",
        "MODEL_NAME = \"runwayml/stable-diffusion-v1-5\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the directory name to save model at.\n",
        "\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/test\" #@param {type:\"string\"}\n",
        "if save_to_gdrive:\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/\" + OUTPUT_DIR\n",
        "else:\n",
        "    OUTPUT_DIR = \"/content/\" + OUTPUT_DIR\n",
        "\n",
        "print(f\"[*] Weights will be saved at {OUTPUT_DIR}\")\n",
        "\n",
        "!mkdir -p $OUTPUT_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----"
      ],
      "metadata": {
        "id": "rHKPwcOsCzLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Optional: BLIP 1 Autocaptioning of datasets"
      ],
      "metadata": {
        "id": "wbP-ENtydnD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "If you don't want to use BLIP, or if your dataset is already labeled, you can skip this step.\n",
        "\n",
        "This section is taken (and modified slightly) from [victorchall](https://github.com/victorchall/EveryDream2trainer#docs)'s EveryDream 2 training notebook.\n",
        "It uses [Salesforce BLIP](https://github.com/salesforce/BLIP) tool to autocaption a given dataset. Captions are saved as the image filenames. These filenames can be extracted to textfiles and used with the Dataset Organization Omnitool below. This is not as accurate as hand-labeling a dataset in most cases, but it's MUCH faster. I plan to implement [BLIP 2](https://github.com/salesforce/LAVIS/blob/main/examples/blip_image_captioning.ipynb) soon.\n"
      ],
      "metadata": {
        "id": "I2xUXWJg-rpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ### Download Repo\n",
        "!git clone https://github.com/victorchall/EveryDream.git\n",
        "# Set working directory\n",
        "%cd EveryDream"
      ],
      "metadata": {
        "id": "bn16ih1hNrGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ###Install Requirements\n",
        "skip_cell_for_run_all()\n",
        "!pip install torch=='1.12.1+cu113' 'torchvision==0.13.1+cu113' --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install pandas>='1.3.5'\n",
        "!git clone https://github.com/salesforce/BLIP scripts/BLIP\n",
        "!pip install timm\n",
        "!pip install fairscale=='0.4.4'\n",
        "!pip install transformers=='4.19.2'\n",
        "!pip install timm\n",
        "!pip install aiofiles"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xfmjkNmLOG7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload your dataset to Google Drive (NOT to the Colab instance--doing this is very slow).\n",
        "Name it something you'll be able to remember/find easily. "
      ],
      "metadata": {
        "id": "9it9Yr81OPEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Auto-Captioning\n",
        "\n",
        "*You cannot have commented lines between uncommented lines.  If you uncomment a line below, move it above any other commented lines.*\n",
        "\n",
        "*!python must remain the first line.*\n",
        "\n",
        "Default params should work fairly well.\n"
      ],
      "metadata": {
        "id": "cpp47gn8OVIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/auto_caption.py \\\n",
        "--img_dir /content/drive/MyDrive/YourDataset \\\n",
        "--out_dir /content/drive/MyDrive/output \\\n",
        "#--format mrwho \\\n",
        "#--min_length 34 \\\n",
        "#--q_factor 1.3 \\\n",
        "#--nucleus \\\n",
        "\n",
        "#IMPORTANT NOTE: replace \"[YourDataset]\" in the --img_dir line with your dataset folder name\n",
        "##ANOTHER NOTE: if you want to save over your original file names instead of making a new directory for your output files,\n",
        "##simply make your output path the same as your input path."
      ],
      "metadata": {
        "id": "1fRNuhXrORpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "That's it! Once your dataset is autocaptioned, you can use these captions in the Dataset Organization Omnitool below!."
      ],
      "metadata": {
        "id": "tkioczpHOb4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------"
      ],
      "metadata": {
        "id": "_1qhNag2C2FX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **KaliYuga's Dataset Organization Omnitool**\n"
      ],
      "metadata": {
        "id": "q0nqbKJQpmz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods of Use\n",
        "\n",
        "###**Method 1: Use Image Filenames As Instance Prompts**\n",
        "####***What Method 1 Does***\n",
        "When you specify the path to your image dataset, running the cell creates text files of each image caption (filename). These are then used instead of instance prompts. \n",
        "\n",
        "**To use this**, simply do not input instance/class prompts or class path below. **You will still need to input an instance_directory path**, as this is the path to all your dataset images.\n",
        "<br></br>\n",
        "\n",
        "###**Method 2: Use Image Filenames alsongside Instance Prompts and Class Prompts**\n",
        "\n",
        "####***What Method 2 Does***\n",
        "Like method one, this method extracts the filenames of all the images in a specified directory and saves them to a text file which can be used as input for machine learning models. Unlike the above section, though, you can use **instance prompts** and **class prompts** alongside basic extracted image captions.\n",
        "<br></br>\n",
        "\n",
        "#####***What Are Instance and Class Prompts?***\n",
        "\n",
        "Instance and class prompts are additional text descriptions of the image content that can help improve the quality of the model's output.\n",
        "\n",
        "Instance prompts describe specific objects or features within an image, such as **\"a red car\"** or **\"a smiling person.\"** Class prompts describe more general concepts or categories, such as **\"a car\"** or **\"a person.\"** By including these prompts in the training data, the model can learn to associate specific features with broader categories, resulting in more accurate and nuanced results. \n",
        "\n",
        "Please note that, if you're using class prompts, you do not have to provide class images yourself. The class prompt you provide will be used to generate these automatically from stable diffusion and save them to your drive.\n",
        "<br></br>\n",
        "#####**IMPORTANT NOTE:**\n",
        "\n",
        "If you have the same word in both your instance and class prompt, it can lead to overfitting on that specific word. When this happens, the model may focus too heavily on that word and generate images that only match that word, rather than the overall concept. To avoid this, it's recommended to choose unique and distinct prompts for both the instance and class.\n",
        "<br></br>\n",
        "\n",
        "---\n",
        "\n",
        "###**Pros and Cons of These Methods of Dataset Labeling**\n",
        "**Pros--**much more accurate text/image pairs than a general instance prompt (assuming good image captioning)\n",
        "\n",
        "**Cons--**hand-captioning datasets is slow going, and tools like BLIP are not always accurate."
      ],
      "metadata": {
        "id": "eQ_HaWz_a6hY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Run"
      ],
      "metadata": {
        "id": "PFjWg7lxbHFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "\n",
        "def extract_image_filenames_to_txts(data_dir):\n",
        "    for subdir, _, files in tqdm(os.walk(data_dir), desc=\"Extracting filenames\"):\n",
        "        for file in files:\n",
        "            if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
        "                filename, extension = os.path.splitext(file)\n",
        "                with open(os.path.join(subdir, f\"{filename}{extension}.txt\"), \"w\") as f:\n",
        "                    f.write(filename)\n",
        "\n",
        "prompts = [\n",
        "   {\n",
        "       \"instance_prompt\":      \"potion\", ## for method 1, you can leave this blank\n",
        "       \"class_prompt\":         \"rpg item\", ## for method 1, you can leave this blank\n",
        "       \"instance_data_dir\":    \"/content/drive/MyDrive/[your image dataset path]\", \n",
        "       \"class_data_dir\":       \"/content/drive/MyDrive/data/[folder to download class images/regularization images from class_prompt ]\", ## for method 1, you can leave this blank\n",
        "   }\n",
        "#     {\n",
        "#         \"instance_prompt\":      \"ukj with a dark-haired woman in Hawaii\",\n",
        "#         \"class_prompt\":         \"photo of a person\",\n",
        "#         \"instance_data_dir\":    \"/content/data/ukj\",\n",
        "#         \"class_data_dir\":       \"/content/data/photosofpeople\"\n",
        "#     }\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    instance_prompt = prompt[\"instance_prompt\"]\n",
        "    class_prompt = prompt[\"class_prompt\"]\n",
        "    data_dir = prompt[\"instance_data_dir\"]\n",
        "    extract_image_filenames_to_txts(data_dir)\n",
        "    for subdir, _, files in os.walk(data_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
        "                filename, extension = os.path.splitext(file)\n",
        "                with open(os.path.join(subdir, f\"{filename}{extension}.txt\"), \"w\") as f:\n",
        "                    f.write(f\"{filename}|{instance_prompt}|{class_prompt}\")\n",
        "                    \n",
        "for c in prompts:\n",
        "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
        "\n",
        "with open(\"concepts_list.json\", \"w\") as f:\n",
        "    json.dump(prompts, f, indent=4)\n"
      ],
      "metadata": {
        "id": "aXw9X9t9gket"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------"
      ],
      "metadata": {
        "id": "sWQXtqN_C8Kw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn5ILIyDJIcX"
      },
      "source": [
        "# Start Training\n",
        "\n",
        "####Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
        "\n",
        "\n",
        "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
        "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
        "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
        "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
        "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
        "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
        "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
        "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
        "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
        "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ioxxvHoicPs"
      },
      "source": [
        "* Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
        "\n",
        "* You must keep `read_prompts_from_txts` flag in order to use the image/text datasets created by the Omnitool.\n",
        "\n",
        "* Remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
        "\n",
        "* Remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality. Not reccomended with this dataset-building method.\n",
        "<br></br>\n",
        "\n",
        "**Notes about training**\n",
        "\n",
        "This method of text/image pair training seems to need a slower `--learning_rate` than other methods or it overfits quickly. `1e-6` is probably the fastest LR you want with most datasets. If it overfits, drop it down into the `ne-7` range. This could be because the prompts and data being used in the datasets created with the Omnitool are more complex/diverse. This could cause the model to require more time and data to converge on a good solution, and therefore need a slower learning rate to avoid overfitting.\n",
        "\n",
        "The default of `1e-7` seems to work for small datasets. It could probably be sped up some, too.\n",
        "\n",
        "If you get an error saying it can't find a specific text file when running the training, check for duplicate images or images with the .gif. It only makes a txt file for the first of two duplicates and not at all for gifs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjcSXTp-u-Eg"
      },
      "outputs": [],
      "source": [
        "!accelerate launch train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
        "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
        "  --output_dir=$OUTPUT_DIR \\\n",
        "  --revision=\"fp16\" \\\n",
        "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
        "  --seed=1337 \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --train_text_encoder \\\n",
        "  --mixed_precision=\"fp16\" \\\n",
        "  --use_8bit_adam \\\n",
        "  --read_prompts_from_txts \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --learning_rate=1e-7 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --num_class_images=200 \\\n",
        "  --sample_batch_size=4 \\\n",
        "  --max_train_steps=80000 \\\n",
        "  --save_interval=500 \\\n",
        "  --save_sample_prompt=\"a beautiful favewave desert nighttime\" \\\n",
        "  --concepts_list=\"concepts_list.json\"\n",
        "# Keep the number of class images close-ish to number of dataset images \n",
        "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
        "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vTCHmHJJpBs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "89Az5NUxOWdy"
      },
      "outputs": [],
      "source": [
        "#@markdown Specify the weights directory to use (leave blank for latest)\n",
        "WEIGHTS_DIR = \"\" #@param {type:\"string\"}\n",
        "if WEIGHTS_DIR == \"\":\n",
        "    from natsort import natsorted\n",
        "    from glob import glob\n",
        "    import os\n",
        "    WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
        "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ma7CpnsLgPX9"
      },
      "outputs": [],
      "source": [
        "#@markdown Run to generate a grid of preview images from the last saved weights.\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "weights_folder = OUTPUT_DIR\n",
        "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
        "\n",
        "row = len(folders)\n",
        "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
        "scale = 4\n",
        "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
        "\n",
        "for i, folder in enumerate(folders):\n",
        "    folder_path = os.path.join(weights_folder, folder)\n",
        "    image_folder = os.path.join(folder_path, \"samples\")\n",
        "    images = [f for f in os.listdir(image_folder)]\n",
        "    for j, image in enumerate(images):\n",
        "        if row == 1:\n",
        "            currAxes = axes[j]\n",
        "        else:\n",
        "            currAxes = axes[i, j]\n",
        "        if i == 0:\n",
        "            currAxes.set_title(f\"Image {j}\")\n",
        "        if j == 0:\n",
        "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
        "        image_path = os.path.join(image_folder, image)\n",
        "        img = mpimg.imread(image_path)\n",
        "        currAxes.imshow(img, cmap='gray')\n",
        "        currAxes.axis('off')\n",
        "        \n",
        "plt.tight_layout()\n",
        "plt.savefig('grid.png', dpi=72)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V8wgU0HN-Kq"
      },
      "source": [
        "## Convert weights to ckpt to use in web UIs like AUTOMATIC1111."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dcXzsUyG1aCy"
      },
      "outputs": [],
      "source": [
        "#@markdown Run conversion.\n",
        "ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n",
        "\n",
        "half_arg = \"\"\n",
        "#@markdown  Whether to convert to fp16, takes half the space (2GB).\n",
        "fp16 = True #@param {type: \"boolean\"}\n",
        "if fp16:\n",
        "    half_arg = \"--half\"\n",
        "!python convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
        "print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToNG4fd_dTbF"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gW15FjffdTID"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
        "from IPython.display import display\n",
        "\n",
        "model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_path, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
        "pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_xformers_memory_efficient_attention()\n",
        "g_cuda = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oIzkltjpVO_f"
      },
      "outputs": [],
      "source": [
        "#@markdown Can set random seed here for reproducibility.\n",
        "g_cuda = torch.Generator(device='cuda')\n",
        "seed = 52362 #@param {type:\"number\"}\n",
        "g_cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "K6xoHWSsbcS3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#@title Run for generating images.\n",
        "\n",
        "prompt = \"\" #@param {type:\"string\"}\n",
        "negative_prompt = \"\" #@param {type:\"string\"}\n",
        "num_samples = 4 #@param {type:\"number\"}\n",
        "guidance_scale = 7.5 #@param {type:\"number\"}\n",
        "num_inference_steps = 24 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"number\"}\n",
        "width = 512 #@param {type:\"number\"}\n",
        "\n",
        "with autocast(\"cuda\"), torch.inference_mode():\n",
        "    images = pipe(\n",
        "        prompt,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        negative_prompt=negative_prompt,\n",
        "        num_images_per_prompt=num_samples,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        generator=g_cuda\n",
        "    ).images\n",
        "\n",
        "for img in images:\n",
        "    display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WMCqQ5Tcdsm2"
      },
      "outputs": [],
      "source": [
        "#@markdown Run Gradio UI for generating images.\n",
        "import gradio as gr\n",
        "\n",
        "def inference(prompt, negative_prompt, num_samples, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
        "    with torch.autocast(\"cuda\"), torch.inference_mode():\n",
        "        return pipe(\n",
        "                prompt, height=int(height), width=int(width),\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_images_per_prompt=int(num_samples),\n",
        "                num_inference_steps=int(num_inference_steps), guidance_scale=guidance_scale,\n",
        "                generator=g_cuda\n",
        "            ).images\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prompt = gr.Textbox(label=\"Prompt\", value=\"photo of zwx dog in a bucket\")\n",
        "            negative_prompt = gr.Textbox(label=\"Negative Prompt\", value=\"\")\n",
        "            run = gr.Button(value=\"Generate\")\n",
        "            with gr.Row():\n",
        "                num_samples = gr.Number(label=\"Number of Samples\", value=4)\n",
        "                guidance_scale = gr.Number(label=\"Guidance Scale\", value=7.5)\n",
        "            with gr.Row():\n",
        "                height = gr.Number(label=\"Height\", value=512)\n",
        "                width = gr.Number(label=\"Width\", value=512)\n",
        "            num_inference_steps = gr.Slider(label=\"Steps\", value=24)\n",
        "        with gr.Column():\n",
        "            gallery = gr.Gallery()\n",
        "\n",
        "    run.click(inference, inputs=[prompt, negative_prompt, num_samples, height, width, num_inference_steps, guidance_scale], outputs=gallery)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lJoOgLQHnC8L"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Delete diffuser and old weights and only keep the ckpt to free up drive space.\n",
        "\n",
        "#@markdown [ ! ] Caution, Only execute if you are sure u want to delete the diffuser format weights and only use the ckpt.\n",
        "import shutil\n",
        "from glob import glob\n",
        "import os\n",
        "for f in glob(OUTPUT_DIR+os.sep+\"*\"):\n",
        "    if f != WEIGHTS_DIR:\n",
        "        shutil.rmtree(f)\n",
        "        print(\"Deleted\", f)\n",
        "for f in glob(WEIGHTS_DIR+\"/*\"):\n",
        "    if not f.endswith(\".ckpt\") or not f.endswith(\".json\"):\n",
        "        try:\n",
        "            shutil.rmtree(f)\n",
        "        except NotADirectoryError:\n",
        "            continue\n",
        "        print(\"Deleted\", f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXgi8HM4c-DA"
      },
      "outputs": [],
      "source": [
        "#@title Free runtime memory\n",
        "exit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "2.7.16 (default, Oct 10 2019, 22:02:15) \n[GCC 8.3.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}